# privacy-sdp
Privacy using Selective Differential Privacy
Introduction
Application of Selective Differential Privacy (SDP) for LLM Privacy Protection
Pals Chinnakannan, Naveed ul Islam, Madhukar Yedulapuram
The rapid growth in Generative Artificial Intelligence around Large Language Models (LLMs), and the advancement in the associated techniques for fine-tuning and prompt tuning have spurred many organizations to customize LLMs for solving business critical problems and for competitive advantage. This growth has resulted in deploying LLMs fine-tuned or prompt-tuned using a wide-range of data containing sensitive Privacy information. However, surveys and studies on the security of LLMs and associated privacy indicate potential scope for vulnerabilities and wide-spread privacy violation on the horizon. These concerns are clearly brought out by Yao et al in [1]. Differential privacy is a technique used to protect PII and other sensitive information through fuzzing such information in a dataset. The essence of differential privacy lies in the injection of "noise" to obscure privacy information present in datasets used in domains like LLM Applications, thereby preserving anonymity while maintaining the overall integrity and the utility of the dataset. Several different Differential Privacy Techniques have been proposed, researched and developed [2], [3]. “Selective Differential Privacy for Large Language Models”, and “Just Fine-Tune Twice: Selective Differential Privacy for Large Language Models” are recent additions to Differential Privacy. SDP is very appropriate for the sparse amount of privacy-related information present in very large datasets like those used in LLM Training and Fine-Tuning. This project proposes to evaluate the Selective Differential Privacy technique as proposed in the paper by W.Shi et al., [5], first by validating the claims in the paper and then by applying those techniques to Meta LLama-2-7b-hf, using the datasets identified in the paper.