{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5619ec11-0df7-441d-891d-611933419b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77b777c-4f91-4c2f-8316-be66f045e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2922dabb-d15c-42de-a6d2-a4a5d638602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ed6138-babc-4b52-9bf4-08713c01053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/Users/pals/MICS/MICS_207/Project/datasets'\n",
    "wikitext2_root = root + '/kaggle-wikitext/wikitext-2/'\n",
    "train_file = wikitext2_root + 'wiki.train.tokens'\n",
    "test_file  = wikitext2_root + 'wiki.test.tokens'\n",
    "valid_file = wikitext2_root + 'wiki.valid.tokens'\n",
    "unittest_file = wikitext2_root + 'unittest.tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be54b6fb-4bdc-497c-9f1c-3f498311b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 1\n",
    "SEQUENCE_LENGTH = 512\n",
    "SHUFFLE_SIZE = 128\n",
    "#BLOCK_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ad7475-27fd-4f0c-bab4-f47db4d0afe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=SEQUENCE_LENGTH):\n",
    "        with open(file_path, 'r', encoding='utf-8') as fd:\n",
    "            self.tokens = []\n",
    "            self.attention_masks = [] # Attention masks\n",
    "            for line in fd:\n",
    "                sline = line.strip()\n",
    "                if len(sline) > 0:\n",
    "                    tokens = tokenizer.encode(sline, truncation=True, max_length=max_length, padding='max_length')\n",
    "                    self.tokens.append(torch.tensor(tokens, dtype=torch.long))\n",
    "                    self.attention_masks.append(torch.tensor([1] * len(tokens), dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.tokens[i], self.attention_masks[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af90fb2-dd5d-4d5d-95cb-75972cc8c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer, shuffle=False, max_length=SEQUENCE_LENGTH, batch_size=BATCH_SIZE):\n",
    "    dataset = TextDataset(file_path, tokenizer, max_length=max_length)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca0c10ca-d99d-4c30-b0ac-4ee634d91c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "#gpt2_tokenizer.padding_side = 'left'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61cb68bc-8b88-4a2c-bedf-b341fad06257",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader    = load_dataset(train_file, gpt2_tokenizer, shuffle=True)\n",
    "test_dataloader     = load_dataset(test_file, gpt2_tokenizer)\n",
    "valid_dataloader    = load_dataset(valid_file, gpt2_tokenizer)\n",
    "unittest_dataloader = load_dataset(unittest_file, gpt2_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15e9aa91-b339-492e-814e-81800fdf15de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[15496,  2159,    13,  ..., 50256, 50256, 50256],\n",
      "        [49488,   318,  2901,  ..., 50256, 50256, 50256],\n",
      "        [34784,  1365,  1110,  ..., 50256, 50256, 50256]]), tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])]\n"
     ]
    }
   ],
   "source": [
    "for ele in unittest_dataloader:\n",
    "    print(ele)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cbc4d28-015b-4c59-8789-f9d4a82914f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "gpt2_lm = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "gpt2_lm.resize_token_embeddings(len(gpt2_tokenizer)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e5989d4-3234-461d-b944-fdcb302c8385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#gpt2_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3d8f4e8-00cb-4491-b27b-2681a4b1c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(gpt2_lm.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4342ce3-8cbe-4142-9706-efb5393df215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "gpt2_lm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b85a243b-a940-4ba5-84a3-566aca4c44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_text, max_length=256):\n",
    "    #device = torch.device(\"cpu\")\n",
    "    gpt2_lm.to(device)\n",
    "    gpt2_lm.eval()\n",
    "    input_ids = gpt2_tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    attention_mask = torch.tensor([1] * len(input_ids[0]), dtype=torch.long).unsqueeze(0).to(device)\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        output = gpt2_lm.generate(input_ids, attention_mask=attention_mask, max_length=max_length, \n",
    "                                  pad_token_id=gpt2_tokenizer.eos_token_id, do_sample=True,\n",
    "                                  num_return_sequences=5,\n",
    "                                  no_repeat_ngram_size=2,\n",
    "                                  temperature=0.7, \n",
    "                                  top_k=50, top_p=0.95)\n",
    "    gen_text = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef64efe4-7ceb-4dd0-9312-b5d4774fc123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pals/MICS/MICS_207/Project/gpt2_pt/lib/python3.10/site-packages/transformers/generation/utils.py:1513: UserWarning: The operator 'aten::isin.Tensor_Tensor_out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  if eos_token_id is not None and torch.isin(elements=eos_token_id, test_elements=pad_token_id).any():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went on a trip to see Tajmahal in Agra. My trip was very successful and it was a very rewarding experience. I am very happy to be here.\n",
      "\n",
      "J.P.: How did you decide to travel to India, and how did your parents decide that you wanted to go to the country?\n",
      " (laughs)\n",
      "...\n",
      ". (laughter) And I think I had a lot of fun in India. It was amazing. And there are many of them. We are all very lucky and privileged to live in a country where people are very keen to come to this country. So I decided to get a chance to visit India and I did. But I was also very surprised by the people and by what was happening there. Many of the stories I heard in the newspapers about people who came to these places were very interesting, but they were not true. There were lots of people. Some of these people were from India who were living in Pakistan, some from Pakistan. One of my friends told me that there were thousands of Indian immigrants to Delhi, who had been living here for a long time and they had never even seen India before. He was just talking about the strange, strange stories of India that they told about these immigrants\n"
     ]
    }
   ],
   "source": [
    "print(generate('I went on a trip to see Tajmahal in Agra. My trip was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "489d67f5-390e-460a-b1e4-9179a76dd34a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, attention_mask in tqdm(dataloader, desc=\"Training gpt2_lm\"):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        outputs = model(inputs, attention_mask=attention_mask, labels=inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40a0ff8f-76af-4294-8541-fb81a2f9aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, attention_mask in tqdm(dataloader, desc=\"Evaluating gpt2_lm\"):\n",
    "            inputs = inputs.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            outputs = model(inputs, attention_mask=attention_mask, labels=inputs)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073297c-9d47-428a-a674-036e208ff2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "743672f1-5930-4ccb-b0dd-a8eec1e0e575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training gpt2_lm: 100%|██████████████████████████████████| 5942/5942 [1:21:10<00:00,  1.22it/s]\n",
      "Evaluating gpt2_lm: 100%|████████████████████████████████████| 616/616 [02:26<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6345567315467673, Validation Loss: 0.5842087776400149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./gpt2_finetuned_pt_v1/tokenizer_config.json',\n",
       " './gpt2_finetuned_pt_v1/special_tokens_map.json',\n",
       " './gpt2_finetuned_pt_v1/vocab.json',\n",
       " './gpt2_finetuned_pt_v1/merges.txt',\n",
       " './gpt2_finetuned_pt_v1/added_tokens.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training loop\n",
    "print(f\"Using device: {device}\")\n",
    "st = time.time()\n",
    "epochs = NUM_EPOCHS\n",
    "train_dl = train_dataloader\n",
    "valid_dl = valid_dataloader\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(gpt2_lm, train_dl, optimizer, device)\n",
    "    valid_loss = evaluate(gpt2_lm, valid_dl, device)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}, Validation Loss: {valid_loss}\")\n",
    "\n",
    "en = time.time()\n",
    "\n",
    "save_path = './gpt2_finetuned_pt_v1'\n",
    "gpt2_lm.save_pretrained(save_path)\n",
    "gpt2_tokenizer.save_pretrained(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4553f8b-084f-483d-bbc6-53165e87f497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47232be6-4062-4e07-9d3a-15a289012fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 1.3937067453066507 hours\n"
     ]
    }
   ],
   "source": [
    "print(f'Training time {(en-st)/3600} hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90c39afa-de0d-4982-ba6c-4a12237218b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I went on a trip to see Tajmahal in Agra. My trip was an amazing experience and I was in the right place at the wrong time. I saw a lot of things, from the Taj Mahal to the <unk> in Kolkata. There were many things that I didn = t see in a few months, but I remember I had an idea that would work. The idea of a tourist attraction in our city was born out of curiosity and curiosity. It was very important to me that there was a place to visit.\n"
     ]
    }
   ],
   "source": [
    "print(generate('I went on a trip to see Tajmahal in Agra. My trip was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6cf402-fd87-4690-b7bf-1985d0e507ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542afef3-aee7-4550-b596-0054b1d23253",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter kernelspec list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264c4ebe-2629-43e7-9049-e6ed7875b944",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter kernelspec uninstall gpt2_pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b5557-97cf-40ff-b01c-9e7878244a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter kernelspec uninstall gpt2_pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12233c6e-3314-4586-ae96-7eaf1bd7d030",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43my\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41298b-0acb-4554-82b7-4d2d0875c14e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
