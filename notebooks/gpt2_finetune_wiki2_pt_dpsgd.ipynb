{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5619ec11-0df7-441d-891d-611933419b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77b777c-4f91-4c2f-8316-be66f045e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2922dabb-d15c-42de-a6d2-a4a5d638602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import s3fs\n",
    "# Imports for DP\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ed6138-babc-4b52-9bf4-08713c01053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 's3://differential-privacy-datasets'\n",
    "wikitext2_root = root + '/kaggle-wikitext/wikitext-2/'\n",
    "train_file = wikitext2_root + 'wiki.train.tokens'\n",
    "test_file  = wikitext2_root + 'wiki.test.tokens'\n",
    "valid_file = wikitext2_root + 'wiki.valid.tokens'\n",
    "unittest_file = wikitext2_root + 'unittest.tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be54b6fb-4bdc-497c-9f1c-3f498311b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 3\n",
    "SEQUENCE_LENGTH = 128\n",
    "SHUFFLE_SIZE = 128\n",
    "#BLOCK_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca0c10ca-d99d-4c30-b0ac-4ee634d91c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "#gpt2_tokenizer.padding_side = 'left'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8ad7475-27fd-4f0c-bab4-f47db4d0afe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=SEQUENCE_LENGTH):\n",
    "        fs = s3fs.S3FileSystem(anon=True)\n",
    "        with fs.open(file_path, 'r', encoding='utf-8') as fd:\n",
    "            self.tokens = []\n",
    "            self.attention_masks = [] # Attention masks\n",
    "            for line in fd:\n",
    "                sline = line.strip()\n",
    "                if len(sline) > 0:\n",
    "                    tokens = tokenizer.encode(sline, truncation=True, max_length=max_length, padding='max_length')\n",
    "                    attention_mask = [1 if token != tokenizer.pad_token_id else 0 for token in tokens]\n",
    "                    self.tokens.append(torch.tensor(tokens, dtype=torch.long))\n",
    "                    self.attention_masks.append(torch.tensor(attention_mask, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.tokens[i], self.attention_masks[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c229121-65a4-4524-9d5e-01075c024747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for padding sequences within a batch to the same length\n",
    "def data_collator(batch):\n",
    "    inputs = [item[0] for item in batch]\n",
    "    attention_masks = [item[1] for item in batch]\n",
    "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=gpt2_tokenizer.pad_token_id)\n",
    "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    return inputs, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af90fb2-dd5d-4d5d-95cb-75972cc8c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer, shuffle=False, max_length=SEQUENCE_LENGTH, batch_size=BATCH_SIZE):\n",
    "    dataset = TextDataset(file_path, tokenizer, max_length=max_length)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61cb68bc-8b88-4a2c-bedf-b341fad06257",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader    = load_dataset(train_file, gpt2_tokenizer, shuffle=True)\n",
    "test_dataloader     = load_dataset(test_file, gpt2_tokenizer)\n",
    "valid_dataloader    = load_dataset(valid_file, gpt2_tokenizer)\n",
    "unittest_dataloader = load_dataset(unittest_file, gpt2_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6679ba8d-8025-475b-b68d-1cc242063a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DP Parameters\n",
    "LEARNING_RATE = 5e-5\n",
    "NOISE_MULTIPLIER = 0.4\n",
    "MAX_GRADIENT_NORM = 0.1\n",
    "PRIVACY_EPSILON = 7.5\n",
    "PRIVACY_DELTA = 1.0 / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15e9aa91-b339-492e-814e-81800fdf15de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2\n",
      "<class 'torch.Tensor'> torch.Size([3, 128])\n",
      "<class 'torch.Tensor'> torch.Size([3, 128])\n"
     ]
    }
   ],
   "source": [
    "for ele in unittest_dataloader:\n",
    "    print(type(ele))\n",
    "    print(len(ele))\n",
    "    for l in ele:\n",
    "        print(type(l), l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cbc4d28-015b-4c59-8789-f9d4a82914f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "gpt2_lm = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "gpt2_lm.resize_token_embeddings(len(gpt2_tokenizer)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4342ce3-8cbe-4142-9706-efb5393df215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "gpt2_lm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b85a243b-a940-4ba5-84a3-566aca4c44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_text, max_length=256):\n",
    "    #device = torch.device(\"cpu\")\n",
    "    gpt2_lm.to(device)\n",
    "    gpt2_lm.eval()\n",
    "    input_ids = gpt2_tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    attention_mask = torch.tensor([1] * len(input_ids[0]), dtype=torch.long).unsqueeze(0).to(device)\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        output = gpt2_lm.generate(input_ids, attention_mask=attention_mask, max_length=max_length, \n",
    "                                  pad_token_id=gpt2_tokenizer.eos_token_id, do_sample=True,\n",
    "                                  num_return_sequences=5,\n",
    "                                  no_repeat_ngram_size=2,\n",
    "                                  temperature=0.7, \n",
    "                                  top_k=50, top_p=0.95)\n",
    "    gen_text = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef64efe4-7ceb-4dd0-9312-b5d4774fc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(generate('I went on a trip to see Tajmahal in Agra. My trip was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f1affbc-9202-4d12-9678-800e27058ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pals/MICS/pt_3.10/lib/python3.10/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/Users/pals/MICS/pt_3.10/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup DP optimizer and PrivacyEngine\n",
    "optimizer_base = torch.optim.AdamW(params=gpt2_lm.parameters(), lr=LEARNING_RATE, eps=1e-8)\n",
    "gpt2_lm.train() # put the model in training mode\n",
    "privacy_engine = PrivacyEngine()\n",
    "\n",
    "gpt2_lm, optimizer, train_dataloader = privacy_engine.make_private_with_epsilon(\n",
    "    module=gpt2_lm,\n",
    "    optimizer=optimizer_base,\n",
    "    data_loader=train_dataloader,\n",
    "    target_epsilon=PRIVACY_EPSILON,\n",
    "    target_delta=PRIVACY_DELTA,\n",
    "    epochs=NUM_EPOCHS, \n",
    "    max_grad_norm=MAX_GRADIENT_NORM,\n",
    "    batch_first = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebee41a3-bcc1-4527-9728-cfc5c3de17ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    losses = []\n",
    "\n",
    "    with BatchMemoryManager(\n",
    "        data_loader=train_dataloader, \n",
    "        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE, \n",
    "        optimizer=optimizer\n",
    "    ) as memory_safe_data_loader:\n",
    "        for step, batch in enumerate(tqdm(memory_safe_data_loader)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                    'attention_mask': batch[1],\n",
    "                    'token_type_ids': batch[2],\n",
    "                    'labels':         batch[3]}\n",
    "\n",
    "            outputs = model(**inputs) # output = loss, logits, hidden_states, attentions\n",
    "\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if step > 0 and step % LOGGING_INTERVAL == 0:\n",
    "                train_loss = np.mean(losses)\n",
    "                eps = privacy_engine.get_epsilon(DELTA)\n",
    "\n",
    "                eval_loss, eval_accuracy = evaluate(model)\n",
    "\n",
    "                print(\n",
    "                  f\"Epoch: {epoch} | \"\n",
    "                  f\"Step: {step} | \"\n",
    "                  f\"Train loss: {train_loss:.3f} | \"\n",
    "                  f\"Eval loss: {eval_loss:.3f} | \"\n",
    "                  f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
    "                  f\"ɛ: {eps:.2f}\"\n",
    "'''    \n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "489d67f5-390e-460a-b1e4-9179a76dd34a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "def train(model, dataloader, optimizer, device, privacy_engine):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    with BatchMemoryManager(data_loader=dataloader, max_physical_batch_size=BATCH_SIZE//2, optimizer=optimizer) as memory_safe_data_loader:\n",
    "        for inputs, attention_mask in tqdm(memory_safe_data_loader, desc=\"Training gpt2_lm with  DPSGD\"):\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            outputs = model(inputs, attention_mask=attention_mask, labels=inputs)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    epsilon, best_alpha = privacy_engine.get_privacy_spent()\n",
    "    print(f\"Privacy budget (ε): {epsilon:.2f}\")\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40a0ff8f-76af-4294-8541-fb81a2f9aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, attention_mask in tqdm(dataloader, desc=\"Evaluating gpt2_lm with DPSGD\"):\n",
    "            inputs = inputs.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            outputs = model(inputs, attention_mask=attention_mask, labels=inputs)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743672f1-5930-4ccb-b0dd-a8eec1e0e575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training gpt2_lm with  DPSGD:   0%|                   | 0/11883 [00:00<?, ?it/s]/Users/pals/MICS/pt_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(f\"Using device: {device}\")\n",
    "#print(f\"Using Model: {gpt2_lm}\")\n",
    "st = time.time()\n",
    "epochs = NUM_EPOCHS\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss = train(gpt2_lm, train_dataloader, optimizer, device, privacy_engine)\n",
    "    valid_loss = evaluate(gpt2_lm, valid_dataloader, device)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}, Validation Loss: {valid_loss}\")\n",
    "\n",
    "en = time.time()\n",
    "\n",
    "#save_path = './gpt2_finetuned_pt_v1'\n",
    "#gpt2_lm.save_pretrained(save_path)\n",
    "#gpt2_tokenizer.save_pretrained(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47232be6-4062-4e07-9d3a-15a289012fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Training time {(en-st)/3600} hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c39afa-de0d-4982-ba6c-4a12237218b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate('I went on a trip to see Tajmahal in Agra. My trip was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6cf402-fd87-4690-b7bf-1985d0e507ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41298b-0acb-4554-82b7-4d2d0875c14e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.10.14:pytorch:MICS/pt_3.10",
   "language": "python",
   "name": "pt_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
