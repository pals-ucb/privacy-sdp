{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5619ec11-0df7-441d-891d-611933419b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f77b777c-4f91-4c2f-8316-be66f045e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2922dabb-d15c-42de-a6d2-a4a5d638602f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.conda/envs/pt_3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import s3fs\n",
    "# Imports for DP\n",
    "from opacus import PrivacyEngine\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ed6138-babc-4b52-9bf4-08713c01053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 's3://differential-privacy-datasets'\n",
    "wikitext2_root = root + '/kaggle-wikitext/wikitext-2/'\n",
    "train_file = wikitext2_root + 'wiki.train.tokens'\n",
    "test_file  = wikitext2_root + 'wiki.test.tokens'\n",
    "valid_file = wikitext2_root + 'wiki.valid.tokens'\n",
    "unittest_file = wikitext2_root + 'unittest.tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be54b6fb-4bdc-497c-9f1c-3f498311b7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 1\n",
    "SEQUENCE_LENGTH = 128\n",
    "SHUFFLE_SIZE = 128\n",
    "#BLOCK_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca0c10ca-d99d-4c30-b0ac-4ee634d91c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "#gpt2_tokenizer.padding_side = 'left'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8ad7475-27fd-4f0c-bab4-f47db4d0afe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length=SEQUENCE_LENGTH):\n",
    "        fs = s3fs.S3FileSystem(anon=True)\n",
    "        with fs.open(file_path, 'r', encoding='utf-8') as fd:\n",
    "            self.tokens = []\n",
    "            self.attention_masks = [] # Attention masks\n",
    "            for line in fd:\n",
    "                sline = line.strip()\n",
    "                if len(sline) > 0:\n",
    "                    tokens = tokenizer.encode(sline, truncation=True, max_length=max_length, padding='max_length')\n",
    "                    attention_mask = [1 if token != tokenizer.pad_token_id else 0 for token in tokens]\n",
    "                    self.tokens.append(torch.tensor(tokens, dtype=torch.long))\n",
    "                    self.attention_masks.append(torch.tensor(attention_mask, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.tokens[i], self.attention_masks[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c229121-65a4-4524-9d5e-01075c024747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for padding sequences within a batch to the same length\n",
    "def data_collator(batch):\n",
    "    inputs = [item[0] for item in batch]\n",
    "    attention_masks = [item[1] for item in batch]\n",
    "    inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True, padding_value=gpt2_tokenizer.pad_token_id)\n",
    "    attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    return inputs, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9af90fb2-dd5d-4d5d-95cb-75972cc8c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path, tokenizer, shuffle=False, max_length=SEQUENCE_LENGTH, batch_size=BATCH_SIZE):\n",
    "    dataset = TextDataset(file_path, tokenizer, max_length=max_length)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61cb68bc-8b88-4a2c-bedf-b341fad06257",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader    = load_dataset(train_file, gpt2_tokenizer, shuffle=True)\n",
    "test_dataloader     = load_dataset(test_file, gpt2_tokenizer)\n",
    "valid_dataloader    = load_dataset(valid_file, gpt2_tokenizer)\n",
    "unittest_dataloader = load_dataset(unittest_file, gpt2_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6679ba8d-8025-475b-b68d-1cc242063a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DP Parameters\n",
    "LEARNING_RATE = 5e-5\n",
    "NOISE_MULTIPLIER = 0.4\n",
    "MAX_GRADIENT_NORM = 0.1\n",
    "PRIVACY_EPSILON = 7.5\n",
    "PRIVACY_DELTA = 1.0 / len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15e9aa91-b339-492e-814e-81800fdf15de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "2\n",
      "<class 'torch.Tensor'> torch.Size([3, 128])\n",
      "<class 'torch.Tensor'> torch.Size([3, 128])\n"
     ]
    }
   ],
   "source": [
    "for ele in unittest_dataloader:\n",
    "    print(type(ele))\n",
    "    print(len(ele))\n",
    "    for l in ele:\n",
    "        print(type(l), l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cbc4d28-015b-4c59-8789-f9d4a82914f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'gpt2'\n",
    "gpt2_lm = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "gpt2_lm.resize_token_embeddings(len(gpt2_tokenizer)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4342ce3-8cbe-4142-9706-efb5393df215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "gpt2_lm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ecf6c4f-cc46-41ef-9e82-7c314863ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_layers(llm, layers):\n",
    "    total_params  = 0\n",
    "    frozen_params = 0\n",
    "\n",
    "    for p in llm.parameters():\n",
    "        p.requires_grad = True\n",
    "        total_params += p.numel()\n",
    "\n",
    "    for layer in layers:\n",
    "        sm = llm.get_submodule(layer)\n",
    "        for _, child in sm.named_modules():\n",
    "            for _, param in child.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    param.requires_grad = False\n",
    "                    frozen_params += param.numel()\n",
    "                \n",
    "    return total_params, frozen_params, total_params-frozen_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4401b3b-f595-4aae-94cf-3bfdb2d5d14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124439808, 117351936, 7087872)\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "        'transformer.h.0', 'transformer.h.1',\n",
    "        'transformer.h.2', 'transformer.h.3',\n",
    "        'transformer.h.4', 'transformer.h.5',\n",
    "        'transformer.h.6', 'transformer.h.7',\n",
    "        'transformer.h.8', 'transformer.h.9',\n",
    "        'transformer.h.10',\n",
    "        'transformer.ln_f',\n",
    "        'transformer.wte', 'transformer.wpe'\n",
    "        ]\n",
    "r = freeze_layers(gpt2_lm, layers)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "204693fb-42d4-4e90-8231-a2921508f5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111.91552734375"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[1]/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ae9289d-59ed-465f-9ecc-3e29ac32031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_requires_grad(module, prefix=''):\n",
    "    \"\"\"\n",
    "    Recursively prints the requires_grad flag for each layer in the model.\n",
    "    \"\"\"\n",
    "    for name, child in module.named_children():\n",
    "        full_name = f\"{prefix}.{name}\" if prefix else name\n",
    "        # Check if the module has parameters\n",
    "        if list(child.parameters()):\n",
    "            for param_name, param in child.named_parameters(recurse=False):\n",
    "                print(f\"Layer: {full_name}.{param_name} - requires_grad: {param.requires_grad}\")\n",
    "        # Recursively check the child module\n",
    "        print_requires_grad(child, full_name)\n",
    "\n",
    "#print_requires_grad(gpt2_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6186743-92d3-49b7-a2c9-c60fd301abbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_layers(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"Layer: {name}, Weights: {param.shape}\")\n",
    "#print_model_layers(gpt2_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b85a243b-a940-4ba5-84a3-566aca4c44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(input_text, max_length=256):\n",
    "    #device = torch.device(\"cpu\")\n",
    "    gpt2_lm.to(device)\n",
    "    gpt2_lm.eval()\n",
    "    input_ids = gpt2_tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    attention_mask = torch.tensor([1] * len(input_ids[0]), dtype=torch.long).unsqueeze(0).to(device)\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        output = gpt2_lm.generate(input_ids, attention_mask=attention_mask, max_length=max_length, \n",
    "                                  pad_token_id=gpt2_tokenizer.eos_token_id, do_sample=True,\n",
    "                                  num_return_sequences=5,\n",
    "                                  no_repeat_ngram_size=2,\n",
    "                                  temperature=0.7, \n",
    "                                  top_k=50, top_p=0.95)\n",
    "    gen_text = gpt2_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef64efe4-7ceb-4dd0-9312-b5d4774fc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(generate('I went on a trip to see Tajmahal in Agra. My trip was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f1affbc-9202-4d12-9678-800e27058ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagemaker-user/.conda/envs/pt_3.10/lib/python3.10/site-packages/opacus/privacy_engine.py:95: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "/home/sagemaker-user/.conda/envs/pt_3.10/lib/python3.10/site-packages/opacus/accountants/analysis/rdp.py:332: UserWarning: Optimal order is the largest alpha. Please consider expanding the range of alphas to get a tighter privacy bound.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Setup DP optimizer and PrivacyEngine\n",
    "optimizer_base = torch.optim.SGD(params=gpt2_lm.parameters(), lr=LEARNING_RATE)\n",
    "gpt2_lm.train() # put the model in training mode\n",
    "privacy_engine = PrivacyEngine()\n",
    "\n",
    "gpt2_lm, optimizer, train_dataloader = privacy_engine.make_private_with_epsilon(\n",
    "    module=gpt2_lm,\n",
    "    optimizer=optimizer_base,\n",
    "    data_loader=train_dataloader,\n",
    "    target_epsilon=PRIVACY_EPSILON,\n",
    "    target_delta=PRIVACY_DELTA,\n",
    "    epochs=NUM_EPOCHS, \n",
    "    max_grad_norm=MAX_GRADIENT_NORM,\n",
    "    batch_first = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4acf7920-0b05-4647-a369-4ebfa54fa6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   28,  5199,  1279,  ..., 50256, 50256, 50256],\n",
      "        [19156,  1279,  2954,  ...,   286,   262, 14576],\n",
      "        [  818,  4793,   837,  ...,   319,   257,   734],\n",
      "        ...,\n",
      "        [  464,  1052,  1279,  ...,   683,   851,   262],\n",
      "        [   27,  2954,    29,  ..., 50256, 50256, 50256],\n",
      "        [  818,  1279,  2954,  ..., 50256, 50256, 50256]])\n",
      "torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i,a in test_dataloader:\n",
    "    print(i)\n",
    "    print(i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "365b09f5-84f6-4d0f-a3ab-d44878a3b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b544337f-f0f4-4527-8595-74f9a907e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dp(model):   \n",
    "    def accuracy(y, y_hat):\n",
    "        return (y == y_hat).mean()\n",
    "        \n",
    "    model.eval()\n",
    "\n",
    "    loss_arr = []\n",
    "    accuracy_arr = []\n",
    "    for inputs, attention_mask in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            inputs = inputs.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            outputs = model(inputs, attention_mask=attention_mask, labels=inputs)\n",
    "            loss, logits = outputs[:2]\n",
    "            preds = np.argmax(logits.detach().cpu().numpy(), axis=2)\n",
    "            labels = inputs.detach().cpu().numpy()\n",
    "            loss_arr.append(loss.item())\n",
    "            accuracy_arr.append(accuracy(preds, labels))\n",
    "    model.train()\n",
    "    return np.mean(loss_arr), np.mean(accuracy_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b692cdde-9343-4b37-a979-b258a9c7a6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "(6.645500314104688, 0.004678670938436564)\n"
     ]
    }
   ],
   "source": [
    "r = evaluate_dp(gpt2_lm)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebee41a3-bcc1-4527-9728-cfc5c3de17ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "from opacus.utils.batch_memory_manager import BatchMemoryManager\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    losses = []\n",
    "\n",
    "    with BatchMemoryManager(\n",
    "        data_loader=train_dataloader, \n",
    "        max_physical_batch_size=MAX_PHYSICAL_BATCH_SIZE, \n",
    "        optimizer=optimizer\n",
    "    ) as memory_safe_data_loader:\n",
    "        for step, batch in enumerate(tqdm(memory_safe_data_loader)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                    'attention_mask': batch[1],\n",
    "                    'token_type_ids': batch[2],\n",
    "                    'labels':         batch[3]}\n",
    "\n",
    "            outputs = model(**inputs) # output = loss, logits, hidden_states, attentions\n",
    "\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if step > 0 and step % LOGGING_INTERVAL == 0:\n",
    "                train_loss = np.mean(losses)\n",
    "                eps = privacy_engine.get_epsilon(DELTA)\n",
    "\n",
    "                eval_loss, eval_accuracy = evaluate(model)\n",
    "\n",
    "                print(\n",
    "                  f\"Epoch: {epoch} | \"\n",
    "                  f\"Step: {step} | \"\n",
    "                  f\"Train loss: {train_loss:.3f} | \"\n",
    "                  f\"Eval loss: {eval_loss:.3f} | \"\n",
    "                  f\"Eval accuracy: {eval_accuracy:.3f} | \"\n",
    "                  f\"ɛ: {eps:.2f}\"\n",
    "'''    \n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "489d67f5-390e-460a-b1e4-9179a76dd34a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "def train(model, dataloader, optimizer, device, privacy_engine):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    with BatchMemoryManager(data_loader=dataloader, max_physical_batch_size=BATCH_SIZE//2, optimizer=optimizer) as memory_safe_data_loader:\n",
    "        for inputs, attention_mask in tqdm(memory_safe_data_loader, desc=\"Training gpt2_lm with  DPSGD\"):\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            outputs = model(inputs, attention_mask=attention_mask, labels=inputs)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    epsilon, best_alpha = privacy_engine.get_privacy_spent()\n",
    "    print(f\"Privacy budget (ε): {epsilon:.2f}\")\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40a0ff8f-76af-4294-8541-fb81a2f9aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, attention_mask in tqdm(dataloader, desc=\"Evaluating gpt2_lm with DPSGD\"):\n",
    "            inputs = inputs.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            outputs = model(inputs, attention_mask=attention_mask, labels=inputs)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743672f1-5930-4ccb-b0dd-a8eec1e0e575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training gpt2_lm with  DPSGD:   0%|                   | 0/11883 [00:00<?, ?it/s]/Users/pals/MICS/pt_3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(f\"Using device: {device}\")\n",
    "#print(f\"Using Model: {gpt2_lm}\")\n",
    "st = time.time()\n",
    "epochs = NUM_EPOCHS\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss = train(gpt2_lm, train_dataloader, optimizer, device, privacy_engine)\n",
    "    valid_loss = evaluate(gpt2_lm, valid_dataloader, device)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}, Validation Loss: {valid_loss}\")\n",
    "\n",
    "en = time.time()\n",
    "\n",
    "#save_path = './gpt2_finetuned_pt_v1'\n",
    "#gpt2_lm.save_pretrained(save_path)\n",
    "#gpt2_tokenizer.save_pretrained(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47232be6-4062-4e07-9d3a-15a289012fa5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Training time {(en-st)/3600} hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c39afa-de0d-4982-ba6c-4a12237218b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate('I went on a trip to see Tajmahal in Agra. My trip was'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6cf402-fd87-4690-b7bf-1985d0e507ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41298b-0acb-4554-82b7-4d2d0875c14e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.10.14:pytorch:MICS/pt_3.10",
   "language": "python",
   "name": "pt_3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
